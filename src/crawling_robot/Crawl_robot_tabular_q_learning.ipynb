{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the Q-Learning algorithm using an $\\epsilon$-greedy policy, discrete states and a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import rpyc\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High level settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should the robots check for distance traveled?\n",
    "#   True: The robot will take constant range mesurments and use them as reward for learning. \n",
    "#         If the measurments go outside the range specified by MOVEMENT_RANGE, any mition will stop until \n",
    "#         the robot is manually moved back to its starting position\n",
    "MOVEMENT_RANGE_CHECK = True\n",
    "# Range in wich it is save to move in. Measured from a wall to the ultra sound sensor at the back of the robot.\n",
    "# Useful for ensuring there is enough space to move in both directions. \n",
    "# Can also be used to prevent the robot from falling of a desk.\n",
    "# Valid range: [0,254]\n",
    "MOVEMENT_RANGE = [20,125]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the robot\n",
    "\n",
    "Perform these steps before attemptiong to connect:\n",
    "\n",
    "1. start the robot\n",
    "2. ensure the connection to the robot is working. Follow tutporial: https://www.ev3dev.org/docs/networking/\n",
    "2. start the rpyc server on the robot. This can be done via command line:\n",
    "    1. ssh into robot\n",
    "```\n",
    "$ ssh robot@ev3dev.lcoal\n",
    "$ Password: maker\n",
    "```\n",
    "    2. start rpyc server script\n",
    "```\n",
    "$ ./rpyc_server.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish connection to rpyc server on robot\n",
    "ROBOT_HOSTNAME = \"ev3dev.local\"\n",
    "conn = rpyc.classic.connect(ROBOT_HOSTNAME)\n",
    "conn.execute(\"print('Hello Slave. I am your master!')\")\n",
    "print(\"sucessfully connected to the robot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robot Implementation\n",
    "The `CrawlerRobot` class is used to control basic movements of the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrawlerRobot:\n",
    "    \"\"\"\n",
    "    Class for managing the one armed crawl robot build by Steffen.\n",
    "    \"\"\"\n",
    "    def __init__(self, connection):\n",
    "        \"\"\" Initialize the robot and calibrate the motors.\n",
    "        \"\"\"\n",
    "        self.conn = connection\n",
    "        motors = connection.modules['ev3dev2.motor'] \n",
    "        self.m1 = motors.LargeMotor('outA')\n",
    "        self.m1.stop_action = 'hold'\n",
    "        self.m2 = motors.LargeMotor('outB')\n",
    "        self.m2.stop_action = 'hold'\n",
    "        sensors = conn.modules['ev3dev2.sensor.lego'] \n",
    "        self.t1 = sensors.TouchSensor('in1')\n",
    "        self.t2 = sensors.TouchSensor('in2')\n",
    "        self.ult = sensors.UltrasonicSensor('in3')\n",
    "        self.m1_pos = -1\n",
    "        self.m2_pos = -1\n",
    "        self.distance = -1\n",
    "        sound = connection.modules['ev3dev2.sound'] \n",
    "        self.sound = sound.Sound()\n",
    "        \n",
    "        self.m1_rotation_levels = {0:-120, 1:-240, 2:-270}\n",
    "        self.m2_rotation_levels = {0:120, 1:190, 2:220}\n",
    "        \n",
    "        self.calibrate()\n",
    "        \n",
    "    def calibrate(self):\n",
    "        \"\"\"calibration function\n",
    "        \"\"\"\n",
    "        # calibrate m1\n",
    "        self.m1.stop_action = 'hold'\n",
    "        self.m1.run_direct(duty_cycle_sp=50) # move m1 up\n",
    "        while True:\n",
    "            if self.t1.is_pressed == True:\n",
    "                self.m1.position = 0\n",
    "                self.m1.reset()\n",
    "                self.m1.stop_action = 'hold'\n",
    "                break\n",
    "\n",
    "        # calibrate m2\n",
    "        self.m2.stop_action = 'coast'\n",
    "        self.m2.run_direct(duty_cycle_sp=-30) # move m1 up\n",
    "        while True:\n",
    "            if self.t2.is_pressed == True:\n",
    "                self.m2.reset()\n",
    "                self.m2.position = 0\n",
    "                break\n",
    "        self.m2.stop_action = 'hold'\n",
    "        \n",
    "        self.m1_pos = -1\n",
    "        self.m2_pos = -1\n",
    "        self.reset_distance()\n",
    "        return\n",
    "    \n",
    "    def move_mot1_to_pos(self, position):\n",
    "        if position != self.m1_pos:\n",
    "            self.m1.on_to_position(20, position=self.m1_rotation_levels[position])\n",
    "            self.m1_pos = position\n",
    "        return\n",
    "    \n",
    "    def move_mot2_to_pos(self, position):\n",
    "        if position != self.m2_pos:\n",
    "            self.m2.on_to_position(20, position=self.m2_rotation_levels[position])\n",
    "            self.m2_pos = position\n",
    "        return\n",
    "    \n",
    "    def reset_distance(self):\n",
    "        \"\"\"resets the distance measure. Call this after manually moving the robot\"\"\"\n",
    "        self.distance = self.ult.distance_centimeters\n",
    "        return\n",
    "        \n",
    "    def move(self, m1_pos, m2_pos):\n",
    "        \"\"\"moves motors to position\n",
    "        \"\"\"\n",
    "        # move motors\n",
    "        self.move_mot1_to_pos(m1_pos)\n",
    "        self.move_mot2_to_pos(m2_pos)\n",
    "        # measure distance\n",
    "        dist_new = self.ult.distance_centimeters\n",
    "        \n",
    "        if MOVEMENT_RANGE_CHECK:\n",
    "            # check for faulty measurments (happens approx 0.1% of the time)\n",
    "            retry = 3\n",
    "            while dist_new > 250 or abs(dist_new - self.distance) > 50:\n",
    "                retry -= 1\n",
    "                if retry <= 0:\n",
    "                    # accept last trusted measurment\n",
    "                    print(\"ignoring new faulty measurment, new: %i, last: %i\" % (dist_new, self.distance))\n",
    "                    dist_new = self.distance\n",
    "                    break\n",
    "                time.sleep(0.5)\n",
    "                dist_new = self.ult.distance_centimeters\n",
    "\n",
    "\n",
    "            # check for enough space remaining\n",
    "            if dist_new > MOVEMENT_RANGE[1] or dist_new < MOVEMENT_RANGE[0]:\n",
    "                self.out_of_space_stop()\n",
    "                dist_new = self.distance\n",
    "            \n",
    "        \n",
    "        dist_traveled = dist_new - self.distance\n",
    "        self.distance = dist_new\n",
    "        \n",
    "        return dist_traveled\n",
    "    \n",
    "    def walk(self):\n",
    "        self.reset_distance()\n",
    "        dist = 0\n",
    "        for i in range(4):\n",
    "            dist += self.move(0,0)\n",
    "            dist += self.move(1,0)\n",
    "            dist += self.move(2,0)\n",
    "            dist += self.move(2,1)\n",
    "            dist += self.move(2,2)\n",
    "            dist += self.move(1,2)\n",
    "            dist += self.move(0,2)\n",
    "            dist += self.move(0,1)\n",
    "        return dist\n",
    "    \n",
    "    def out_of_space_stop(self):\n",
    "        self.notify(\"Ran out of space. Move me and press button to restart\")\n",
    "        while True:\n",
    "            if self.t1.is_pressed == True:\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "            if self.t2.is_pressed == True:\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "        time.sleep(0.5)\n",
    "        self.reset_distance()\n",
    "        return\n",
    "        \n",
    "    def notify(self, text):\n",
    "        self.sound.speak(text)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment Implementation\n",
    "\n",
    "The `CrawlerEnv` implements and `openAI` Gym interface for the crawling robot learning problem. This abstraction layer allows using standardized reinforcement learning code for learning. It also allows us to test the same learning implementation on other (simulated) `openAI` enviroments. \n",
    "\n",
    "This implementation loosly based on the cliff walking gym: https://github.com/podondra/gym-gridworlds/blob/master/gym_gridworlds/envs/cliff_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrawlerEnv(gym.Env):\n",
    "    metadata = {'render.modes': []}\n",
    "    \n",
    "    def __init__(self):\n",
    "        # states of each motor\n",
    "        self.m0_states = 3\n",
    "        self.m1_states = 3\n",
    "        \n",
    "        # action space\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # state space\n",
    "        self.observation_space = spaces.Tuple((\n",
    "                spaces.Discrete(self.m0_states),\n",
    "                spaces.Discrete(self.m1_states)\n",
    "                ))\n",
    "        self.observation_space.n = self.m0_states * self.m1_states\n",
    "        \n",
    "        # reward space\n",
    "        self.reward_range = [- math.inf, math.inf]\n",
    "        \n",
    "        # action encodings\n",
    "        self.moves = {\n",
    "                0: (-1, 0),   # m0 down\n",
    "                1: (1, 0),   # m0 up\n",
    "                2: (0, -1),  # m1 down\n",
    "                3: (0, 1),  # m1 up\n",
    "                }\n",
    "        \n",
    "        # connect to robot\n",
    "        self.robot = CrawlerRobot(conn)\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. When end of\n",
    "        episode is reached, you are responsible for calling `reset()`\n",
    "        to reset this environment's state.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        Args:\n",
    "            action (object): an action provided by the environment\n",
    "        Returns:\n",
    "            observation (object): agent's observation of the current environment\n",
    "            reward (float) : amount of reward returned after previous action\n",
    "            done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        # calculate new state\n",
    "        x, y = self.moves[action]\n",
    "        self.state = self.state[0] + x, self.state[1] + y\n",
    "        \n",
    "        reward = 0\n",
    "\n",
    "        # keep states in allowed Box\n",
    "        for arm in [0,1]:\n",
    "            if self.state[arm] < 0:\n",
    "                reward -= 3\n",
    "        self.state = max(0, self.state[0]), max(0, self.state[1])\n",
    "        self.state = (min(self.state[0], self.m0_states - 1),\n",
    "                      min(self.state[1], self.m1_states - 1))\n",
    "\n",
    "        # perform movement\n",
    "        reward += self.robot.move(*self.state)\n",
    "        \n",
    "        return self.state_as_int, reward, False, {}\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the state of the environment and returns an initial observation.\n",
    "        \n",
    "        Returns: observation (object): the initial observation of the\n",
    "            space.\n",
    "        \"\"\"\n",
    "        self.robot.reset_distance()\n",
    "        self.robot.move(0, 0)\n",
    "        self.state = (0, 0)\n",
    "        return self.state_as_int\n",
    "    \n",
    "    \n",
    "    def render(self, mode, close=False):\n",
    "        \"\"\"Renders the environment.\n",
    "        \"\"\"\n",
    "        # raise NotImplementedError\n",
    "        super(CrawlerEnv, self).render(mode=mode)\n",
    "    \n",
    "    @property\n",
    "    def state_as_int(self):\n",
    "        return self.state[0] + self.m0_states * self.state[1]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create enviroment, Calibrate\n",
    "\n",
    "This creates the training enviroment and calibrates the motors of the robot.\n",
    "\n",
    "### ! Ensure that the motors calibrate properly. Once hitting the buttons, they should not bounce back too much  !\n",
    "Repeat the calibration process multiple times if neccessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CrawlerEnv()\n",
    "number_of_actions = env.action_space.n\n",
    "number_of_states = env.observation_space.n\n",
    "print(\"|S| =\", number_of_states)\n",
    "print(\"|A| =\", number_of_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test deterministic movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.robot.walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.robot.move(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of tabular Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(f_name=\"model.p\"):\n",
    "    \"\"\" saves the current model parameters as the given file name\n",
    "    \"\"\"\n",
    "    model_params = {\"weights\":trained_weights,\n",
    "                    \"epsilon\":epsilon,\n",
    "                    \"eta\" : eta,\n",
    "                    \"R_list\" : R_list,\n",
    "                    \"E_list\" : E_list,\n",
    "                    \"iter\" : len(R_list),\n",
    "                    \"gamma\" : gamma,\n",
    "                    \"initial_epsilon\" : initial_epsilon\n",
    "                   }\n",
    "    pickle.dump( model_params, open( f_name, \"wb\" ) )\n",
    "    return\n",
    "\n",
    "def new_model():\n",
    "    \"\"\" creates a new model with default parameters\n",
    "    \"\"\"\n",
    "    return {\"weights\":None,\n",
    "            \"epsilon\":0.5,\n",
    "            \"eta\" : 0.5,\n",
    "            \"R_list\" : [],\n",
    "            \"E_list\" : [],\n",
    "            \"iter\" : 0,\n",
    "            \"gamma\" : 0.8\n",
    "           }\n",
    "\n",
    "def load_model(f_name=\"model.p\", learning=False):\n",
    "    \"\"\" loads a model from the given file. Sets exploration and learning rate to 0.\n",
    "    \"\"\"\n",
    "    model = pickle.load( open( f_name, \"rb\" ) )\n",
    "    if not learning:\n",
    "        model[\"epsilon\"] = 0\n",
    "        model[\"eta\"] = 0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define linear model\n",
    "# y = W^T x     ,y is R^4 for 4 actions, x is R^48 for 48 states, W is R^48x4\n",
    "\n",
    "# one-hot encoding of current state s\n",
    "x = tf.placeholder(shape=[None, number_of_states],dtype=tf.float32)  # input state\n",
    "\n",
    "# weights of the Q(s, a) function\n",
    "W = tf.Variable(tf.zeros([number_of_states, number_of_actions]))\n",
    "\n",
    "# y = values for all 4 actions in state s, represents Q(s, . )\n",
    "y = tf.matmul(x, W) \n",
    "\n",
    "# get best action\n",
    "argmax_y = tf.argmax(y, 1)\n",
    "\n",
    "#  Loss is mesured difference between  target and predicted Q values.\n",
    "# target = values to learn\n",
    "target = tf.placeholder(shape=[None, number_of_actions], dtype=tf.float32)\n",
    "lr = tf.placeholder(dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(target - y))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate = lr)\n",
    "train_step = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load a model or create a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = new_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"./model_1_perfect/model_30min.p\", learning=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epsilon = epsilon = model[\"epsilon\"]   # epsilon for epsilon-greedy selection\n",
    "number_of_episodes = 100\n",
    "max_number_of_steps = 35\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Set learning parameters\n",
    "gamma = model[\"gamma\"]  # gamma\n",
    "eta = model[\"eta\"]  # (initial) learning rate\n",
    "R_list = model[\"R_list\"]  # list gathering returns (accumulated rewards) for each episode\n",
    "E_list = model[\"E_list\"]  # list gathering exploration factors\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if not (model[\"weights\"] is None):\n",
    "        W.load(model[\"weights\"])\n",
    "    for i in tqdm(range(number_of_episodes)):\n",
    "        s = env.reset()  # reset environment and get first state\n",
    "        R = 0  # return (accumulated reward)\n",
    "        for t in range(max_number_of_steps):  # maximum number of steps\n",
    "            # Choose an action greedily (with e chance of random action) from the Q-network, based on current state\n",
    "            # x: one-hot encoding of current state\n",
    "            # a is a skalar represented as a list eg: [1]\n",
    "            # Q is current approximation of the action-value function, vector over all states\n",
    "            a, Q = sess.run([argmax_y, y], feed_dict={x:np.eye(1, number_of_states, s)}) \n",
    "            if np.random.rand(1) < epsilon:\n",
    "                # epsilon-greedy with random exploration here\n",
    "                a[0] = env.action_space.sample()\n",
    "            # Observe new state and reward from environment\n",
    "            # s_prime = new state\n",
    "            # r = reward of action\n",
    "            # d = boolean, true if terminal state reached\n",
    "            s_prime, r, d, _ = env.step(a[0])\n",
    "            # Compute Q' by feeding the new state into the network, get vector of values of actions of new state\n",
    "            Q_prime = sess.run(y, feed_dict={x:np.eye(1, number_of_states, s_prime)})\n",
    "            # Compute maximum value of Q_prime and set  target value for chosen action\n",
    "            max_Q_prime = np.max(Q_prime)  # get maximum value of new state\n",
    "            Q_target = Q\n",
    "            Q_target[0, a[0]] = r + gamma * max_Q_prime  # update Q by updating the value of the action sampled\n",
    "            # Train network using target and predicted Q values\n",
    "            _ = sess.run([train_step], feed_dict={x:np.eye(1, number_of_states, s), target:Q_target, lr:eta})\n",
    "            R += r\n",
    "            s = s_prime\n",
    "            if t >= max_number_of_steps - 1:  # episode end\n",
    "                # Reduce probability of random actions over time\n",
    "                if initial_epsilon > 0.:\n",
    "                    epsilon = 1./((i/10) + (1./initial_epsilon))\n",
    "                    eta = 1./((i/10) + (1./initial_epsilon))\n",
    "                break\n",
    "        \n",
    "        R_list.append(R) # reward accumulated in episode\n",
    "        E_list.append(epsilon) # exploration factor\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            # take model snapshot\n",
    "            trained_weights = sess.run(W)\n",
    "            save_model(\"model_%s_epocs.p\" % i)\n",
    "        \n",
    "    # save weights\n",
    "    trained_weights = sess.run(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(\"model.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(weights):\n",
    "    def best_action(actions):\n",
    "        a = np.argmax(actions)\n",
    "        if a == 0:\n",
    "            return \"m1 up   \"\n",
    "        elif a == 1:\n",
    "            return \"m1 down \"\n",
    "        elif a == 2:\n",
    "            return \"m2 away \"\n",
    "        elif a == 3:\n",
    "            return \"m2 close\"\n",
    "        \n",
    "    m1_pos = {0: \"up     \", 1: \"mid    \", 2: \"down   \"}\n",
    "    m2_pos = {0: \"away   \", 1: \"mid    \", 2: \"close  \"}\n",
    "        \n",
    "    print(\"M1 pos   | M2 pos   | best action | w m1 up  | m1 down  | m2 away  | m2 close \")\n",
    "    print(\"---------|----------|-------------|----------|----------|----------|----------\")\n",
    "    for i in range(9):\n",
    "        print(\" %s | %s  | %s    | %6.3f   | %6.3f   | %6.3f   | %6.3f  \" % (m1_pos[(int(i%3))], m2_pos[(int(i/3))], best_action(weights[i,:]), weights[i,0], weights[i,1], weights[i,2], weights[i,3]))\n",
    "        \n",
    "        \n",
    "visualize_weights(trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(R_list, 'g.')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title(\"Reward gathered\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(E_list, 'r')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Exploration factor')\n",
    "plt.title(\"Exploration factor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = []\n",
    "for i in range(100):\n",
    "    xs.append(1./((i/10) + (1./initial_epsilon)))\n",
    "    \n",
    "plt.plot(xs, 'r')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Exploration factor')\n",
    "plt.title(\"Exploration factor\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
